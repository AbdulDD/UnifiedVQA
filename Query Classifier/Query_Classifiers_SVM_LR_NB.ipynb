{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# # Read the CSV files\n",
        "# csv1 = \"/content/Queries_Dataset.csv\"  # Replace with the path to your first CSV file\n",
        "# csv2 = \"/content/selected_questions.csv\"  # Replace with the path to your second CSV file\n",
        "\n",
        "# data1 = pd.read_csv(csv1)\n",
        "\n",
        "# print(len(data1))\n",
        "# data2 = pd.read_csv(csv2)\n",
        "\n",
        "# # Rename column 'Target' to 'target' in the second dataframe\n",
        "# if 'Target' in data2.columns:\n",
        "#     data2.rename(columns={'Target': 'target'}, inplace=True)\n",
        "\n",
        "# # Append the dataframes\n",
        "# data_combined = pd.concat([data1, data2], ignore_index=True)\n",
        "\n",
        "# # Shuffle the rows randomly\n",
        "# data_shuffled = data_combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# # Save the shuffled data to a new CSV\n",
        "# output_csv = \"shuffled_output.csv\"  # Replace with the desired output file name\n",
        "# data_shuffled.to_csv(output_csv, index=False)\n",
        "\n",
        "# print(f\"Shuffled data saved to {output_csv}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6f3Uxaqkyf0",
        "outputId": "be68ec65-581f-4696-e726-faf6171f2859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n",
            "Shuffled data saved to shuffled_output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import joblib  # Import joblib for saving models\n"
      ],
      "metadata": {
        "id": "PZ3AlxYeejWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 1: Load the dataset\n",
        "data = pd.read_csv('/content/shuffled_output.csv')\n",
        "\n",
        "# Step 2: Split the dataset into train, test, and validation sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)  # 80% train, 20% test\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)  # Split train into 90% train, 10% validation\n",
        "\n",
        "# Step 3: Prepare the data using TF-IDF vectorization\n",
        "tfidf = TfidfVectorizer(max_features=5000)  # Limit to top 5000 features\n",
        "\n",
        "# Fit and transform on training data, then transform on validation and test data\n",
        "X_train_tfidf = tfidf.fit_transform(train_data['Question'])\n",
        "X_val_tfidf = tfidf.transform(val_data['Question'])\n",
        "X_test_tfidf = tfidf.transform(test_data['Question'])\n",
        "\n",
        "y_train = train_data['target']\n",
        "y_val = val_data['target']\n",
        "y_test = test_data['target']\n"
      ],
      "metadata": {
        "id": "gNLZmtTlemvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg = LogisticRegression(max_iter=1000, C=0.1)  # Adjust C for regularization\n",
        "\n",
        "svm = LinearSVC()\n",
        "nb = MultinomialNB()"
      ],
      "metadata": {
        "id": "WXuuICVQoBYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Helper function to track metrics at each iteration\n",
        "def track_metrics(model, X_train, y_train, X_val, y_val, epochs):\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.partial_fit(X_train, y_train, classes=np.unique(y_train))  # Update model\n",
        "\n",
        "        # Train predictions\n",
        "        train_preds = model.predict(X_train)\n",
        "        train_probs = model.predict_proba(X_train)\n",
        "        train_loss = log_loss(y_train, train_probs)\n",
        "        train_acc = accuracy_score(y_train, train_preds)\n",
        "\n",
        "        # Validation predictions\n",
        "        val_preds = model.predict(X_val)\n",
        "        val_probs = model.predict_proba(X_val)\n",
        "        val_loss = log_loss(y_val, val_probs)\n",
        "        val_acc = accuracy_score(y_val, val_preds)\n",
        "\n",
        "        # Track losses and accuracies\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        print(f\"Train Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
        "        print(f\"Train Accuracy: {train_acc}, Validation Accuracy: {val_acc}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies\n"
      ],
      "metadata": {
        "id": "DT9iPcZgpbyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Logistic Regression via SGDClassifier\n",
        "logreg_sgd = SGDClassifier(loss='log_loss', max_iter=1, warm_start=True)  # Logistic regression\n",
        "train_losses_logreg, val_losses_logreg, train_acc_logreg, val_acc_logreg = track_metrics(\n",
        "    logreg_sgd, X_train_tfidf, y_train, X_val_tfidf, y_val, epochs=10\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcRu-WVdqMWH",
        "outputId": "70fd40b1-bb60-4f5e-cb72-c8accad01e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Train Loss: 0.05065367225280855, Validation Loss: 0.05478695377967704\n",
            "Train Accuracy: 0.999537037037037, Validation Accuracy: 1.0\n",
            "--------------------------------------------------\n",
            "Epoch 2/10\n",
            "Train Loss: 0.05290854971768224, Validation Loss: 0.0564500550129149\n",
            "Train Accuracy: 0.9990740740740741, Validation Accuracy: 1.0\n",
            "--------------------------------------------------\n",
            "Epoch 3/10\n",
            "Train Loss: 0.05449088559330984, Validation Loss: 0.058777773766297114\n",
            "Train Accuracy: 0.9990740740740741, Validation Accuracy: 1.0\n",
            "--------------------------------------------------\n",
            "Epoch 4/10\n",
            "Train Loss: 0.0536714349321995, Validation Loss: 0.05835989179293263\n",
            "Train Accuracy: 0.9990740740740741, Validation Accuracy: 1.0\n",
            "--------------------------------------------------\n",
            "Epoch 5/10\n",
            "Train Loss: 0.053941735930894776, Validation Loss: 0.05780949421076968\n",
            "Train Accuracy: 0.9990740740740741, Validation Accuracy: 1.0\n",
            "--------------------------------------------------\n",
            "Epoch 6/10\n",
            "Train Loss: 0.05494656495945527, Validation Loss: 0.060375434781319966\n",
            "Train Accuracy: 0.9990740740740741, Validation Accuracy: 1.0\n",
            "--------------------------------------------------\n",
            "Epoch 7/10\n",
            "Train Loss: 0.05470630314201949, Validation Loss: 0.06044758928824331\n",
            "Train Accuracy: 0.9990740740740741, Validation Accuracy: 1.0\n",
            "--------------------------------------------------\n",
            "Epoch 8/10\n",
            "Train Loss: 0.05387630628023343, Validation Loss: 0.05991587086945214\n",
            "Train Accuracy: 0.9990740740740741, Validation Accuracy: 1.0\n",
            "--------------------------------------------------\n",
            "Epoch 9/10\n",
            "Train Loss: 0.05438882355187131, Validation Loss: 0.060939733126761396\n",
            "Train Accuracy: 0.9990740740740741, Validation Accuracy: 1.0\n",
            "--------------------------------------------------\n",
            "Epoch 10/10\n",
            "Train Loss: 0.05436839762910691, Validation Loss: 0.060996638735911636\n",
            "Train Accuracy: 0.9990740740740741, Validation Accuracy: 1.0\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import hinge_loss, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Adjust learning rate if needed by tuning `eta0`\n",
        "svm_sgd = SGDClassifier(loss='hinge', max_iter=1, eta0=0.01, learning_rate='constant')\n",
        "\n",
        "def track_metrics_svm(model, X_train, y_train, X_val, y_val, epochs):\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Shuffle the data at the beginning of each epoch\n",
        "        X_train, y_train = shuffle(X_train, y_train, random_state=epoch)\n",
        "\n",
        "        model.partial_fit(X_train, y_train, classes=np.unique(y_train))  # Update model\n",
        "\n",
        "        # Train predictions\n",
        "        train_preds = model.decision_function(X_train)\n",
        "        train_loss = hinge_loss(y_train, train_preds)\n",
        "        train_acc = accuracy_score(y_train, model.predict(X_train))\n",
        "\n",
        "        # Validation predictions\n",
        "        val_preds = model.decision_function(X_val)\n",
        "        val_loss = hinge_loss(y_val, val_preds)\n",
        "        val_acc = accuracy_score(y_val, model.predict(X_val))\n",
        "\n",
        "        # Track losses and accuracies\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        print(f\"SVM Train Loss (Hinge): {train_loss}, Validation Loss (Hinge): {val_loss}\")\n",
        "        print(f\"SVM Train Accuracy: {train_acc}, Validation Accuracy: {val_acc}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
        "\n",
        "# Assuming X_train_tfidf, y_train, X_val_tfidf, and y_val are already defined\n",
        "train_losses_svm, val_losses_svm, train_acc_svm, val_acc_svm = track_metrics_svm(\n",
        "    svm_sgd, X_train_tfidf, y_train, X_val_tfidf, y_val, epochs=10\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kba30J7nqjzA",
        "outputId": "24e8bb38-f09d-4bf7-a450-d3145389ab2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "SVM Train Loss (Hinge): 0.40909811606026003, Validation Loss (Hinge): 0.40702759352472345\n",
            "SVM Train Accuracy: 0.8912037037037037, Validation Accuracy: 0.8625\n",
            "--------------------------------------------------\n",
            "Epoch 2/10\n",
            "SVM Train Loss (Hinge): 0.26256073642890193, Validation Loss (Hinge): 0.2867128217683144\n",
            "SVM Train Accuracy: 0.9222222222222223, Validation Accuracy: 0.8833333333333333\n",
            "--------------------------------------------------\n",
            "Epoch 3/10\n",
            "SVM Train Loss (Hinge): 0.19259759796620904, Validation Loss (Hinge): 0.21510329414716445\n",
            "SVM Train Accuracy: 0.9532407407407407, Validation Accuracy: 0.925\n",
            "--------------------------------------------------\n",
            "Epoch 4/10\n",
            "SVM Train Loss (Hinge): 0.1388173309640285, Validation Loss (Hinge): 0.1619571484512893\n",
            "SVM Train Accuracy: 0.9777777777777777, Validation Accuracy: 0.9625\n",
            "--------------------------------------------------\n",
            "Epoch 5/10\n",
            "SVM Train Loss (Hinge): 0.09760276659708061, Validation Loss (Hinge): 0.11688661973648319\n",
            "SVM Train Accuracy: 0.9902777777777778, Validation Accuracy: 0.9833333333333333\n",
            "--------------------------------------------------\n",
            "Epoch 6/10\n",
            "SVM Train Loss (Hinge): 0.06764188816495141, Validation Loss (Hinge): 0.07986464587709735\n",
            "SVM Train Accuracy: 0.9944444444444445, Validation Accuracy: 0.9916666666666667\n",
            "--------------------------------------------------\n",
            "Epoch 7/10\n",
            "SVM Train Loss (Hinge): 0.04810801723183066, Validation Loss (Hinge): 0.05521014869507917\n",
            "SVM Train Accuracy: 0.9953703703703703, Validation Accuracy: 1.0\n",
            "--------------------------------------------------\n",
            "Epoch 8/10\n",
            "SVM Train Loss (Hinge): 0.03453886108847698, Validation Loss (Hinge): 0.03898150943043432\n",
            "SVM Train Accuracy: 0.9967592592592592, Validation Accuracy: 1.0\n",
            "--------------------------------------------------\n",
            "Epoch 9/10\n",
            "SVM Train Loss (Hinge): 0.025133605589896472, Validation Loss (Hinge): 0.02719903947576545\n",
            "SVM Train Accuracy: 0.9972222222222222, Validation Accuracy: 1.0\n",
            "--------------------------------------------------\n",
            "Epoch 10/10\n",
            "SVM Train Loss (Hinge): 0.018722070066422118, Validation Loss (Hinge): 0.019243761389028186\n",
            "SVM Train Accuracy: 0.9972222222222222, Validation Accuracy: 1.0\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Naive Bayes (MultinomialNB) training with tracking\n",
        "nb = MultinomialNB()\n",
        "train_losses_nb, val_losses_nb, train_acc_nb, val_acc_nb = track_metrics(\n",
        "    nb, X_train_tfidf, y_train, X_val_tfidf, y_val, epochs=10\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mjw8BavMqmlm",
        "outputId": "c591a364-2cd9-4940-f2c2-2aaaa01e797a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Train Loss: 0.10642100160927634, Validation Loss: 0.1317635342352663\n",
            "Train Accuracy: 0.9986111111111111, Validation Accuracy: 0.9666666666666667\n",
            "--------------------------------------------------\n",
            "Epoch 2/10\n",
            "Train Loss: 0.05677904513877317, Validation Loss: 0.09125140881540651\n",
            "Train Accuracy: 0.999537037037037, Validation Accuracy: 0.9666666666666667\n",
            "--------------------------------------------------\n",
            "Epoch 3/10\n",
            "Train Loss: 0.03761897181243509, Validation Loss: 0.0747892689389664\n",
            "Train Accuracy: 0.999537037037037, Validation Accuracy: 0.9666666666666667\n",
            "--------------------------------------------------\n",
            "Epoch 4/10\n",
            "Train Loss: 0.02769490683306599, Validation Loss: 0.06571518935381784\n",
            "Train Accuracy: 1.0, Validation Accuracy: 0.9666666666666667\n",
            "--------------------------------------------------\n",
            "Epoch 5/10\n",
            "Train Loss: 0.02173285439290335, Validation Loss: 0.05992358758463288\n",
            "Train Accuracy: 1.0, Validation Accuracy: 0.9708333333333333\n",
            "--------------------------------------------------\n",
            "Epoch 6/10\n",
            "Train Loss: 0.01780444234206755, Validation Loss: 0.0558870011549709\n",
            "Train Accuracy: 1.0, Validation Accuracy: 0.9708333333333333\n",
            "--------------------------------------------------\n",
            "Epoch 7/10\n",
            "Train Loss: 0.015045479530617483, Validation Loss: 0.052902492157207215\n",
            "Train Accuracy: 1.0, Validation Accuracy: 0.9708333333333333\n",
            "--------------------------------------------------\n",
            "Epoch 8/10\n",
            "Train Loss: 0.013014709898716281, Validation Loss: 0.05060015625726181\n",
            "Train Accuracy: 1.0, Validation Accuracy: 0.975\n",
            "--------------------------------------------------\n",
            "Epoch 9/10\n",
            "Train Loss: 0.011465046228903644, Validation Loss: 0.04876628279915061\n",
            "Train Accuracy: 1.0, Validation Accuracy: 0.975\n",
            "--------------------------------------------------\n",
            "Epoch 10/10\n",
            "Train Loss: 0.010248200697755021, Validation Loss: 0.047268554222839904\n",
            "Train Accuracy: 1.0, Validation Accuracy: 0.975\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWnxfzC0edT1",
        "outputId": "123a4b6f-f0cc-4c65-c982-745db1464f49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Validation Accuracy: 0.9875\n",
            "SVM Validation Accuracy: 1.0\n",
            "Naive Bayes Validation Accuracy: 0.9666666666666667\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 4: Train and evaluate models\n",
        "\n",
        "# Logistic Regression\n",
        "\n",
        "logreg.fit(X_train_tfidf, y_train)\n",
        "y_val_pred_logreg = logreg.predict(X_val_tfidf)\n",
        "print(f\"Logistic Regression Validation Accuracy: {accuracy_score(y_val, y_val_pred_logreg)}\")\n",
        "\n",
        "# SVM (LinearSVC)\n",
        "\n",
        "svm.fit(X_train_tfidf, y_train)\n",
        "y_val_pred_svm = svm.predict(X_val_tfidf)\n",
        "print(f\"SVM Validation Accuracy: {accuracy_score(y_val, y_val_pred_svm)}\")\n",
        "\n",
        "# Naive Bayes (MultinomialNB)\n",
        "\n",
        "nb.fit(X_train_tfidf, y_train)\n",
        "y_val_pred_nb = nb.predict(X_val_tfidf)\n",
        "print(f\"Naive Bayes Validation Accuracy: {accuracy_score(y_val, y_val_pred_nb)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 5: Test the models on the test set\n",
        "print(\"\\nTest Set Performance:\")\n",
        "y_test_pred_logreg = logreg.predict(X_test_tfidf)\n",
        "print(f\"Logistic Regression Test Accuracy: {accuracy_score(y_test, y_test_pred_logreg)}\")\n",
        "\n",
        "y_test_pred_svm = svm.predict(X_test_tfidf)\n",
        "print(f\"SVM Test Accuracy: {accuracy_score(y_test, y_test_pred_svm)}\")\n",
        "\n",
        "y_test_pred_nb = nb.predict(X_test_tfidf)\n",
        "print(f\"Naive Bayes Test Accuracy: {accuracy_score(y_test, y_test_pred_nb)}\")\n",
        "\n",
        "# Step 6: Evaluate the Logistic Regression model (or you can choose another)\n",
        "print(\"\\nLogistic Regression Evaluation Metrics:\")\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred_logreg))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_test_pred_logreg))\n",
        "\n",
        "print(\"\\nSVM Evaluation Metrics:\")\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred_svm))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_test_pred_svm))\n",
        "\n",
        "print(\"\\nNiave bayes Evaluation Metrics:\")\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred_nb))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_test_pred_nb))\n",
        "\n",
        "# Step 7: Save the trained models\n",
        "joblib.dump(logreg, '/content/logistic_regression_model.pkl')\n",
        "joblib.dump(svm, '/content/svm_model.pkl')\n",
        "joblib.dump(nb, '/content/naive_bayes_model.pkl')\n",
        "joblib.dump(tfidf, '/content/tfidf_vectorizer.pkl')  # Save the vectorizer as well\n",
        "\n",
        "print(\"\\nModels saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3d8Z1JTerGY",
        "outputId": "bcf17525-f637-47ed-b57c-d44e6fe562dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Set Performance:\n",
            "Logistic Regression Test Accuracy: 0.9866666666666667\n",
            "SVM Test Accuracy: 0.9933333333333333\n",
            "Naive Bayes Test Accuracy: 0.985\n",
            "\n",
            "Logistic Regression Evaluation Metrics:\n",
            "Confusion Matrix:\n",
            " [[195   1   0]\n",
            " [  0 212   7]\n",
            " [  0   0 185]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00       196\n",
            "           1       1.00      0.97      0.98       219\n",
            "           2       0.96      1.00      0.98       185\n",
            "\n",
            "    accuracy                           0.99       600\n",
            "   macro avg       0.99      0.99      0.99       600\n",
            "weighted avg       0.99      0.99      0.99       600\n",
            "\n",
            "\n",
            "SVM Evaluation Metrics:\n",
            "Confusion Matrix:\n",
            " [[195   1   0]\n",
            " [  0 216   3]\n",
            " [  0   0 185]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00       196\n",
            "           1       1.00      0.99      0.99       219\n",
            "           2       0.98      1.00      0.99       185\n",
            "\n",
            "    accuracy                           0.99       600\n",
            "   macro avg       0.99      0.99      0.99       600\n",
            "weighted avg       0.99      0.99      0.99       600\n",
            "\n",
            "\n",
            "Niave bayes Evaluation Metrics:\n",
            "Confusion Matrix:\n",
            " [[196   0   0]\n",
            " [  1 210   8]\n",
            " [  0   0 185]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00       196\n",
            "           1       1.00      0.96      0.98       219\n",
            "           2       0.96      1.00      0.98       185\n",
            "\n",
            "    accuracy                           0.98       600\n",
            "   macro avg       0.98      0.99      0.99       600\n",
            "weighted avg       0.99      0.98      0.98       600\n",
            "\n",
            "\n",
            "Models saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Load the saved models and vectorizer\n",
        "logreg_model = joblib.load('/content/logistic_regression_model.pkl')\n",
        "svm_model = joblib.load('/content/svm_model.pkl')\n",
        "nb_model = joblib.load('/content/naive_bayes_model.pkl')\n",
        "tfidf_vectorizer = joblib.load('/content/tfidf_vectorizer.pkl')\n",
        "\n",
        "# Function to make predictions\n",
        "def predict_query_type(question):\n",
        "    # Transform the input question using the TF-IDF vectorizer\n",
        "    question_tfidf = tfidf_vectorizer.transform([question])\n",
        "\n",
        "    # Make predictions with each model\n",
        "    logreg_prediction = logreg_model.predict(question_tfidf)[0]\n",
        "    svm_prediction = svm_model.predict(question_tfidf)[0]\n",
        "    nb_prediction = nb_model.predict(question_tfidf)[0]\n",
        "\n",
        "    # Return the predictions\n",
        "    return {\n",
        "        'Logistic Regression Prediction': logreg_prediction,\n",
        "        'SVM Prediction': svm_prediction,\n",
        "        'Naive Bayes Prediction': nb_prediction\n",
        "    }\n",
        "\n",
        "# Example question\n",
        "question = 'What is status of pie driver in stock?'\n",
        "predictions = predict_query_type(question)\n",
        "\n",
        "# Print the predictions\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myDkE3WkfFKd",
        "outputId": "1200d7d6-76b9-4129-f217-8024bb18ad75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Logistic Regression Prediction': 2, 'SVM Prediction': 2, 'Naive Bayes Prediction': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dfctBobxfkCH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}